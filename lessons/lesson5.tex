\addcontentsline{toc}{chapter}{Занятие 5. Винеровский процесс}
\chapter*{Занятие 5. Винеровский процесс}

\addcontentsline{toc}{section}{Контрольные вопросы и задания}
\section*{Контрольные вопросы и задания}

\subsubsection*{Приведите определение винеровского процесса.}

$ \left\{ w \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс,
если обладает рядом свойств:
\begin{enumerate}
  \item $w \left( 0 \right) = 0$;
  \item однородные приращения.
  Рассмотрим приращение винеровского процесса на $t$.
  Тогда
  $w \left( s + t \right) - w \left( s \right) \overset{def}{=}
    w \left( t \right) \sim
    N \left( 0, t \right) $, то есть распределение процесса зависит только от длины отрезка;
  \item независимые приращения на непересекающихся отрезках.
  Выберем $0 < t_1 < t_2 < \dotsc < t_n$.
  Тогда
  $w \left( t_1 \right), \,
    w \left( t_2 \right) - w \left( t_1 \right),
    \dotsc,
    w \left( t_n \right) - w \left( t_{n - 1} \right) $ ---
  независимые в совокупности случайные величины.
\end{enumerate}

\subsubsection*{Запишите плотность винеровского процесса.}

Напишем плотность распределения вектора
$ \left( w \left( t_1 \right), \dotsc, w \left( t_n \right) \right) =
  \vec{ \xi }$.

Будем использовать матрицу
$$A =
  \begin{bmatrix}
    1 & 0 & 0 & 0 & \dotsc & 0 \\
    1 & 1 & 0 & 0 & \dotsc & 0 \\
    1 & 1 & 1 & 0 & \dotsc & 0 \\
    \dotsc \\
    1 & 1 & 1 & 1 & \dotsc & 1
  \end{bmatrix}.$$

Таким образом $ \vec{ \xi }$ имеет плотность
$$q \left( A^{-1} \vec{u} \right) =
  \prod \limits_{j = 0}^{n - 1}
    \frac{1}{ \sqrt{2 \pi \left( t_{j + 1} - t_j \right) }} \cdot e^{-\frac{u_{j + 1} - u_j}{2t_{j + 1} - t_j}}.$$
В этой плотности считаем, что $t_0 = 0, \, u_0 = 0$.

\subsubsection*{Запишите ковариационную функцию винеровского процесса.}

Произведение математических ожиданий --- это 0, потому
$$K \left( t, s \right) =
  Mw \left( s \right) w \left( t \right) =$$
Используем независимость приращений
$$= M \left\{
    w \left( s \right) \cdot
    \left[ w \left( s \right) + \left( w \left( t \right) - w \left( s \right) \right) \right]
  \right\} =$$
Раскрываем скобки
$$= M \left\{
    w^2 \left(s \right) + Mw \left( s \right) \left[ w \left( t \right) - w \left( s \right) \right]
  \right\} =$$
Первое слагаемое равно $s$, а второе --- нулю,
так как это независимые центрированные случайные величины (математическое произведения ---
это произведение математических ожиданий, а они равны нулю)
$$= s, \,
  s < t.$$

$K \left( t, s \right) = \min \left( s, t \right) $.

\addcontentsline{toc}{section}{Аудиторные задачи}
\section*{Аудиторные задачи}

\subsubsection*{5.2}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Докажите, что
$M \left( W \left( t \right) - W \left( s \right) \right)^{2n + 1} = 0, \,
  M \left( W \left( t \right) - W \left( s \right) \right)^{2n} =
  \left( 2n - 1 \right)!! \left( t - s \right)^n$.

\textit{Решение.}
Приращение гауссовское.
Обозначим
$$ \xi =
  W \left( t \right) - W \left( s \right) \overset{def}{=}
  W \left( t - s \right).$$
Значит, $ \xi \sim N \left( 0, t - s \right) $, где $t - s = \sigma^2$.
Нужны формулы для моментов центрированной гауссовской случайной величины, то есть
Знаем, что $M \xi^{2n + 1} = 0, \, M \xi^{2n} = \left( 2n + 1 \right)!! \sigma^{2n}$.

\subsubsection*{5.3}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Вычислите:
\begin{enumerate}[label=\alph*)]
  \item $M \left[ \left( W \left( 5 \right) - 2W \left( 1 \right) + 2 \right)^3 \right] $;
  \item характеристическую функцию случайной величины $W \left( 2 \right) + 2W \left( 1 \right) $;
  \item $M \left[ \sin \left( 2W \left( 1 \right) + W \left( 2 \right) \right) \right] $;
  \item $M \left[ \cos \left( 2W \left( 1 \right) + W \left( 2 \right) \right) \right] $.
\end{enumerate}

\textit{Решение.}
Есть винеровский процесс.
\begin{enumerate}[label=\alph*)]
  \item $W \left( 5 \right) - 2W \left( 1 \right) + 2 = \xi \sim N \left( 2, 5 \right) $,
  потому что это линейная комбинация элементов гауссовского вектора.
  Найдём дисперсию.
  Константа на неё не влияет
  $$D \xi =
    D \left[ W \left( 5 \right) - 2W \left( 1 \right) \right] =
    cov \left( \xi, \xi \right) =$$
  Подставим выражения для случайной величины
  $$= cov \left[ W \left( 5 \right) - 2W \left( 1 \right) + 2, \,
      W \left( 5 \right) - 2W \left( 1 \right) + 2 \right] =$$
  Воспользуемся линейностью
  $$= K \left( 5, 5 \right) - 2K \left( 5, 1 \right) - 2K \left( 5, 1 \right) +
    4K \left( 1, 1 \right) =
    5 - 2 - 2 + 4 =
    5.$$
  Нужно найти третий момент.
  $ \xi $ не центрирована.
  Нужно её центрировать $M \xi^3 = M \left[ \left( \xi - 2 \right) + 2 \right]^3 $.
  Раскрываем скобки
  $$M \xi^3 =
    M \left( \xi - 2 \right)^3 + 6M \left( \xi - 2 \right)^3 + 12M \left( \xi - 2 \right) + 8.$$
  По предыдущей задаче первое слагаемое --- 0, так как величина центрирована, второй момент --- 5,
  так как это дисперсия, первый момент --- 0.
  Тогда $M \xi^3 = 0 + 6 \cdot 5 + 12 \cdot 0 + 8 = 38$.

  Величины $W \left( 5 \right) $ и $W \left( 1 \right) $ --- зависимы,
  а приращения в винеровском процессе --- независимы, потому имеем сумму дисперсий
  $$D \left[ W \left( 5 \right) - 2W \left( 1 \right) \right] =
    D \left\{
      \left[ W \left( 5 \right) - W \left( 1 \right) \right] + \left[ -W \left( 1 \right) \right]
    \right\}.$$
  Дисперсия первого слагаемого равна 4, а второго --- 1.
  Слагаемые независимы $D \left[ W \left( 5 \right) - 2W \left( 1 \right) \right] = 5$;
  \item нужно найти характеристическую функцию $W \left( 2 \right) + 2W \left( 1 \right) $.

  Математическое ожидание такой величины равно нулю, а дисперсия
  $D \left[ W \left( 2 \right) + 2W \left( 1 \right) \right] =
    D \left\{
      \left[ W \left( 2 \right) - W \left( 1 \right) \right] + 3W \left( 1 \right)
    \right\}$.
  Это независимые величины, поэтому
  $D \left\{
      \left[ W \left( 2 \right) - W \left( 1 \right) \right] + 3W \left( 1 \right)
    \right\} =
    1 + 9 = 10$.
  Значит, получается
  $ \varphi_{W \left( 2 \right) + 2W \left( 1 \right) } \left( \lambda \right) =
    \varphi_{N \left( 0, 10 \right) } \left( \lambda \right) =
    e^{-\frac{10 \lambda^2}{2}}$;
  \item $M \left[ \sin \left( 2W \left( 1 \right) + W \left( 2 \right) \right) \right] = 0$.

  Характеристическая функция случайной величины --- это
  $$ \varphi_{ \xi } \left( \lambda \right) = Me^{i \lambda \xi } =
    M \cos \lambda \xi + iM \sin \lambda \xi, \,
    \lambda = 1;$$
  \item $M \left[ \cos \left( 2W \left( 1 \right) + W \left( 2 \right) \right) \right] = e^{-5}$.
\end{enumerate}

\subsubsection*{5.4}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Докажите, что процессы
\begin{enumerate}[label=\alph*)]
  \item $ \left\{ -W \left( t \right), \, t \geq 0 \right\} $;
  \item $ \left\{ W \left( s + t \right) - W \left( s \right), \, t \geq 0 \right\} $;
  \item $ \tilde{W} \left( t \right) =
    tW \left( \frac{1}{t} \right) \cdot \mathbbm{1} \left\{ t > 0 \right\} $
\end{enumerate}
тоже являются винеровскими.

\textit{Решение.}
$ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- это винеровский процесс.
Нужно проверить, что некоторые преобразования винеровского процесса оставляют его винеровским.

\begin{enumerate}[label=\alph*)]
  \item Если выберем моменты времени $t_1 < \dotsc < t_n$ и возьмём вектор
  $$ \left( W \left( t_1 \right), \dotsc, W \left( t_n \right) \right) $$
  --- гауссовский.
  Нужно знать, что в каждой точке $MW \left( t \right) = 0$ и
  $$K \left( t, s \right) =
    \min \left( t, s \right).$$
  Если процесс удовлетворит этим трём свойствам, то это винеровский процесс.

  $M \left[ -W \left( t \right) \right] =
    -MW \left( t \right) =
    0$.

  Найдём ковариационную функцию
  $$K \left( t, s \right) =
    M \left[ W \left( t \right) W \left( s \right) \right] =
    \min \left( t, s \right).$$

  Вектор значений этого процесса должен быть гауссовским.
  Возьмём $ \left( -W \left( t_1 \right), \dotsc, -W \left( t_n \right) \right) $.
  Нужно сказать, что это гауссовский вектор.
  Почему?

  Этот вектор --- это линейное преобразование вектора
  $$ \left( W \left( t_1 \right), \dotsc, W \left( t_n \right) \right).$$
  Линейные преобразования оставляют вектор гауссовским;
  \item сначала нужно сказать, что у него гауссовские конечномерные распределения.

  Берём $n$ значений этого процесса
  $$ \left(
      W \left( s + t_1 \right) - W \left( s \right), \dotsc,
      W \left( s + t_n \right) - W \left( s \right)
    \right) $$
  --- гауссовский, так как этот вектор --- это линейное преобразование вектора
  $ \left( W \left( t_1 + s \right), \dotsc, W \left( t_n + s \right), W \left( s \right) \right) $.
  Что это будет за линейное преобразование?

  $$ \begin{bmatrix}
      1 & 0 & -1 \\
      0 & 1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      W \left( s + t_1 \right) \\
      W \left( s + t_2 \right) \\
      W \left( s \right)
    \end{bmatrix} =
    \begin{bmatrix}
      W \left( s + t_1 \right) - W \left( s \right) \\
      W \left( s + t_2 \right) - W \left( s \right)
    \end{bmatrix}.$$

  Математическое ожидание --- 0.

  Нужно посчитать ковариационную функцию.
  Нужно проверить, что она равняется минимуму
  $$K \left( t_1, t_2 \right) =
    M \left\{
      \left[ W \left( s + t_1 \right) - W \left( s \right) \right] \cdot
      \left[ W \left( s + t_2 \right) - W \left( s \right) \right] \right\} =$$
  Перемножим скобки
  $$= M \left[
      W \left( s + t_1 \right) W \left( s + t_2 \right) -
      W \left( s + t_1 \right) W \left( s \right) - W \left( s \right) W \left( s + t_2 \right) +
      W \left( s \right)^2 \right] =$$
  Математическое ожидание первого слагаемого --- ковариация винеровского процесса.
  Она равна минимуму.
  Математическое ожидание последнего слагаемого --- ковариация в точке $ \left( s, s \right) $.
  Получаем
  $$= \min \left( s + t_1, s + t_2 \right) - s - s + s =
    \min \left( s + t_1, s + t_2 \right) - s.$$
  Можем вынести и сократить
  $\min \left( s + t_1, s + t_2 \right) - s =
    \min \left( t_1, t_2 \right) $.
  Значит, ковариация такая, как надо.
  Это винеровский процесс;
  \item берём конечномерные распределения
  $$ \left(
      t_1 W \left( \frac{1}{t_1} \right), \dotsc, t_n W \left( \frac{1}{t_n} \right)
    \right) $$
  --- гауссовский, так как это линейное преобразование вектора винеровского процесса
  $ \left( W \left( \frac{1}{t_1} \right), \dotsc, W \left( \frac{1}{t_n} \right) \right) $.

  Математическое ожидание --- 0.
  Осталось найти ковариационную функцию
  $$K \left( t, s \right) =
    M \left[ tW \left( \frac{1}{t} \right) sW \left( \frac{1}{s} \right) \right] =$$
  Выносим $t$ и $s$.
  Получаем
  $$= ts \min \left( \frac{1}{t}, \frac{1}{s} \right) =$$
  Множитель $ts$ --- положительный.
  Он вносится
  $$= \min \left( t, s \right).$$
  Получилось.
\end{enumerate}

\subsubsection*{5.5}

\textit{Задание.}
Пусть $ \left\{ W^i \left( t \right), \, t \geq 0 \right\}_{i \geq 1}$ ---
независимые винеровские процессы.
Найдите константу $c_n$ так, чтобы процесс
$$ \tilde{W} \left( t \right) =
  c_n \sum \limits_{i = 1}^n W^i \left( t \right), \,
  t \geq 0$$
был винеровским.

\textit{Решение.}
Сложили $n$ независимых винеровских процессов так, чтобы процесс был винеровским.

Скажем, что такой процесс гауссовский
\begin{gather*}
  \left( \tilde{W} \left( t_1 \right), \dotsc, \tilde{W} \left( t_n \right) \right) = \\
  = \left(
    c_n \left( W^1 \left( t_1 \right), \dotsc, W^n \left( t_n \right) \right), \dotsc,
    c_n \left( W^1 \left( t_m \right), \dotsc, W^n \left( t_m \right) \right)
  \right)
\end{gather*}
--- это линейное преобразование.

$$ \begin{bmatrix}
    W^1 \left( t_1 \right) \\
    \dotsc \\
    W^1 \left( t_m \right) \\
    W^2 \left( t_1 \right) \\
    \dotsc \\
    W^2 \left( t_m \right)
  \end{bmatrix}$$
--- гауссовский вектор, где обе части --- независимые гауссовские вектора.

Математическое ожидание такого процесса --- 0, так как математическое ожидание каждого процесса ---
0.
Посчитаем ковариацию и скажем, какой должна быть $c_n.$
Ковариация линейна по каждому аргументу.
Это значит, что множители и суммы выносятся
$$cov \left(
    c_n \sum \limits_{i = 1}^n W^i \left( t \right), \,
    c_n \sum \limits_{i = 1}^n W^i \left( s \right)
  \right) =
  c_n^2 \sum \limits_{i = 1}^n
    \sum \limits_{j = 1}^n cov \left[ W^i \left( t \right), \, W^j \left( s \right) \right] =$$
Когда индексы разные --- это 0, когда одинаковые --- это минимум
$$= c_n^2 \sum \limits_{i = 1}^n \min \left( t, s \right) =$$
Имеем $n$ одинаковых слагаемых
$$= c_n^2 \cdot n \cdot \min \left( t, s \right).$$

Отсюда получаем
$$c_n =
  \frac{1}{ \sqrt{n}},$$
тогда процесс винеровский.

\subsubsection*{5.6}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Для $0 < t \leq s$ вычислите вероятность
$q_t =
  P \left( W \left( s \right) > W \left( s - t \right) > W \left( s + t \right) \right) $.

\textit{Решение.}
Начнём с того, что нарисуем график винеровского процесса (рис. \ref{fig:56}).

\begin{figure}[h!]
  \centering
  \includegraphics[width=.4\textwidth]{./pictures/5_6.png}
  \caption{График винеровского процесса}
  \label{fig:56}
\end{figure}

Есть 3 случайные величины.

У такого вектора есть плотность.
Случайные величины независимы
$$q_t =
  \iiint \limits_{x > y > z}
    p_{ \left( W \left( s \right), W \left( s - t \right), W \left( s + t \right) \right) }
      \left( x, y, z \right) dxdydz.$$

Вектор имеет нормальное распределение
$$ \begin{bmatrix}
    W \left( s \right) \\
    W \left( s - t \right) \\
    W \left( s + t \right)
  \end{bmatrix} \sim
  N \left(
    \left( \begin{bmatrix}
      0 \\
      0 \\
      0
    \end{bmatrix} \right),
    \left( \begin{bmatrix}
      s & s - t & s \\
      s - t & s - t & s - t \\
      s & s - t & s + t
    \end{bmatrix} \right) \right).$$

Нужно использовать какие-то свойства винеровского процесса.
Здесь нужно взять 2 приращения.
Эти приращения будут независимыми величинами с известным распределением $N \left( 0, t \right) $.

Вводим в рассмотрение приращения
$$ \begin{cases}
    X = W \left( s \right) - W \left( s - t \right), \\
    Y = W \left( s + t \right) - W \left( s \right).
  \end{cases}$$
Выразим из первого уравнения $W \left( s - t \right) = W \left( s \right) - X$,
а из второго --- $W \left( s + t \right) = W \left( s \right) + Y$.
Отнимем два последние уравнения
$$W \left( s + t \right) - W \left( s - t \right) =
  X + Y.$$
От всех частей неравенства в искомой вероятности вычтем $W \left( s - t \right) $
и заменим полученные выражения на введённые приращения
$$q_t =
  P \left\{
    W \left( s \right) - W \left( s - t \right) > 0 >
    W \left( s + t \right) - W \left( s - t \right) \right\} =
  P \left( X > 0 > X + Y \right).$$
Плотность вектора --- это произведение плотностей
$$P \left( X > 0 > X + Y \right) =
  \int \limits_{0}^{ \infty } \int \limits_{- \infty }^{-x}
    \frac{1}{2 \pi t} \cdot e^{-\frac{1}{2t} \left( x^2 + y^2 \right) } dydx =$$
Перейдём в полярную систему координат
$$x = r \cos \varphi, \,
  y = r \sin \varphi, \,
  dxdy = rdrd \varphi.$$
Получим
$$= \frac{1}{2 \pi t} \int \limits_0^{ \infty }
  \int \limits_{-\frac{ \pi }{2}}^{ \frac{ \pi }{4}} re^{-\frac{1}{2t} \cdot r^2} d \varphi dr =$$
Изобразим область интегрирования (рис. \ref{fig:561}).

\begin{figure}[h!]
  \centering
  \includegraphics[width=.4\textwidth]{./pictures/5_6_1.png}
  \caption{Область интегрирования}
  \label{fig:561}
\end{figure}

По $ \varphi $ можем сразу проинтегрировать.
Интеграл по $ \varphi $ даст просто $ \frac{ \pi }{4}$.
Получаем
$$= \frac{1}{8} \int \limits_0^{ \infty } e^{-\frac{1}{2t} \cdot r^2} \cdot \frac{dr^2}{2t} =$$
Интеграл равен единице
$$= \frac{1}{8}$$

\subsubsection*{5.7}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Найдите математическое ожидание и ковариационную функцию процессов:
\begin{enumerate}[label=\alph*)]
  \item $W^0 \left( t \right) = W \left( t \right) - tW \left( 1 \right), \, 0 \leq t \leq 1$
  (броуновский мост);
  \item $U \left( t \right) = e^{-\frac{t}{2}} W \left( e^t \right) $ (процесс Орнштейна-Уленбека).
\end{enumerate}
Выясните, какой из этих процессов является гауссовским.

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item $MW^0 \left( t \right) = 0$, потому что у винеровского процесса математическое ожидание 0.
  Найдём ковариационную функцию
  $$K \left( t, s \right) =
    cov \left[
      W \left( t \right) - tW \left( 1 \right), W \left( s \right) - sW \left( 1 \right) \right] =
    \min \left( t, s \right) - ts - st + st =$$
  Одинаковые слагаемые с разными знаками уничтожаются
  $$= \min \left( t, s \right) - st.$$

  Если возьмём вектор конечномерных распределений
  $$ \left( W^0 \left( t_1 \right), \dotsc, W^0 \left( t_n \right) \right),$$
  то этот вектор будет гауссовским.
  Такой процесс называется броуновский мост (рис. \ref{fig:57});

  \begin{figure}[h!]
    \centering
    \includegraphics[width=.4\textwidth]{./pictures/5_7.png}
    \caption{Броуновский мост}
    \label{fig:57}
  \end{figure}

  \item $MU \left( t \right) = 0$, потому что винеровский.
  Ковариационная функция
  $$K \left( t, s \right) =
    cov \left[
      e^{-\frac{t}{2}} W \left( e^t \right), e^{-\frac{s}{2}} W \left( e^s \right) \right] =$$
  Выносим экспоненты (множители)
  $$= e^{-\frac{t}{2} - \frac{s}{2}} \min \left( e^t, e^s \right) =$$
  Экспонента --- монотонная функция
  $$= e^{-\frac{t}{2} - \frac{s}{2} + \min \left( t, s \right) } =
    e^{-\frac{1}{2} \left[ t + s - 2 \min \left( s, t \right) \right] } =
    e^{-\frac{1}{2} \cdot \left| t - s \right| }.$$
  Значение процесса $U \left( t \right) $ ---
  это линейное преобразование значений винеровского процесса, только в других точках.
  Процесс гауссовский.
\end{enumerate}

\subsubsection*{5.8}

\textit{Задание.}
Докажите, что случайный процесс
$$B \left( t \right) = \left( 1 - t \right) W \left( \frac{t}{1 - t} \right), \,
  0 \leq t < 1; \,
  B \left( 1 \right) = 0$$
имеет то же распределение, что и броуновский мост.

\textit{Решение.}
Конечномерные распределения такого процесса
$$ \left( B \left( t_1 \right), \dotsc, B \left( t_n \right) \right) $$
--- гауссовские вектора,
потому что это линейное преобразование винеровского процесса.

$MB \left( t \right) = 0$.

Найдём ковариацию
$$cov \left[ B \left( t \right), B \left( s \right) \right] =
  cov \left[
    \left( 1 - t \right) W \left( \frac{t}{1 - t} \right),
    \left( 1 - s \right) W \left( \frac{s}{1 - s} \right) \right] =$$
Множители выносим
$$= \left( 1 - t \right) \left( 1 - s \right)
  \min \left( \frac{t}{1 - t}, \frac{s}{1 - s} \right) =$$
Вносим положительный множитель в минимум
$$= \min \left( t - ts, s - ts \right) =$$
Общее выносим за минимум
$$= \min \left( t, s \right) - ts,$$
то есть ковариация такая же, как и у броуновского моста.

\subsubsection*{5.9}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Вычислите условное математическое ожидание
$M \left( \left. W \left( s \right) \right| W \left( t \right) \right) $ при $s > t$.

\textit{Решение.}
Есть формула
$$ \frac{ \int \limits_{-\infty }^{+\infty } xp_{W \left( s \right), W \left( t \right) } \left( x, y \right) dx}{ \int \limits_{-\infty }^{+\infty } p_{W \left( s \right), W \left( t \right) } \left( x, y \right) dx}.$$

Свойства условного математического ожидания:
$M \left( \left. \xi \right| \mathcal{F} \right) = \xi $,
если $ \xi $ измерима относительно $ \mathcal{F}$ и
$M \left( \left. \xi \right| \mathcal{F} \right) = M \xi $,
если $ \xi $ не зависит от $ \mathcal{F}$.

Нужно, чтобы появилось приращение
$$M \left[ \left. W \left( s \right) \right| W \left( t \right) \right] =
  M \left[
    \left. W \left( s \right) - W \left( t \right) + W \left( t \right) \right| W \left( t \right)
  \right] =$$
Распишем как 2 условных математических ожидания
$$= M \left[ \left. W \left( s \right) - W \left( t \right) \right| W \left( t \right) \right] +
  M \left[ \left. W \left( t \right) \right| W \left( t \right) \right].$$
Первое слагаемое равно нулю, как как имеются независимые величины
$M \left[ \left. W \left( s \right) - W \left( t \right) \right| W \left( t \right) \right] +
  M \left[ \left. W \left( t \right) \right| W \left( t \right) \right] =
  W \left( t \right) $.

\subsubsection*{5.10}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс,
и пусть $ \tau $ --- независимая от процесса $W$ случайная величина,
показательно распределённая с параметром $ \lambda $.
Найдите характеристическую функцию случайной величины $W \left( \tau \right) $.

\textit{Решение.}
$ \varphi_{W \left( \tau \right) } \left( \lambda \right) =
  Me^{i \lambda W \left( \tau \right) }$.
В винеровский процесс подставляется случайное время.
Похожая ситуация
$$Me^{i \lambda \sum \limits_{k = 0}^{ \tau } \xi_k} =
  \sum \limits_{k = 0}^{ \infty }
    P \left( \tau = k \right) \cdot
    M \left( \left. e^{i \lambda \sum \limits_{k = 0}^{ \tau } \xi_k} \right| \tau = k \right).$$
Получаем
$$Me^{i \lambda W \left( \tau \right) } =
  MM \left[ \left. e^{i \lambda W \left( \tau \right) } \right| \tau \right] =
  Me^{-\frac{ \lambda^2}{2} \cdot \tau^2 } =
  \int \limits_{ \mathbb{R}} e^{-\frac{ \lambda^2 x}{2}} p_{ \tau } \left( x \right) dx =$$
Подставим выражение для плотности показательного распределения
$$= \int \limits_0^{+\infty } e^{-\frac{ \lambda^2 x}{2}} \lambda e^{-\lambda x} dx =
  \lambda \int \limits_0^{+\infty } e^{-\frac{ \lambda^2 x}{2} - \lambda x} dx =
  \lambda \int \limits_0^{+ \infty } e^{-x \left( \frac{ \lambda^2}{2} + \lambda \right) } dx =$$
Вынесем $ \lambda $ за скобки
$$= \lambda \int \limits_0^{+ \infty } e^{-x \lambda \left( \frac{ \lambda }{2} + 1 \right) } dx =
  \lambda \int \limits_0^{+ \infty } e^{-x \lambda \cdot \frac{ \lambda + 2}{2}} dx =
  -\lambda \cdot \frac{2}{ \lambda + 2} \cdot \frac{1}{ \lambda }
  \left. e^{-\lambda x \cdot \frac{ \lambda + 2}{2}} \right|_0^{+ \infty } =
  \frac{2}{ \lambda + 2}.$$

\addcontentsline{toc}{section}{Домашнее задание}
\section*{Домашнее задание}

\subsubsection*{5.12}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Вычислите:
\begin{enumerate}[label=\alph*)]
  \item $M \left[
      \left( W \left( 4 \right) - 2W \left( 1 \right) + 2W \left( 2 \right) \right)^2 \right] $;
  \item $M \left[ \left( W \left( 1 \right) + 2W \left( 2 \right) + 1 \right)^3 \right] $;
  \item $M \left[ e^{W \left( 3 \right) - 2W \left( 2 \right) } \right] $;
  \item характеристическую функцию случайной величины $W \left( 1 \right) + 2W \left( 2 \right) + 1$.
\end{enumerate}

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item $W \left( 4 \right) - 2W \left( 1 \right) + W \left( 2 \right) =
    \xi \sim
    N \left( 0, 6 \right) $,
  потому что это линейная комбинация элементов гауссовского вектора.
  Найдём дисперссию
  $$D \xi =
    D \left[ W \left( 4 \right) - 2W \left( 1 \right) + W \left( 2 \right) \right] =
    cov \left( \xi, \xi \right) =$$
  Подставим выражения для случайной величины
  $$= cov \left[
      W \left( 4 \right) - 2W \left( 1 \right) + W \left( 2 \right),
      W \left( 4 \right) - 2W \left( 1 \right) + W \left( 2 \right) \right] =$$
  Воспользуемся линейностью
  \begin{gather*}
    = K \left( 4, 4 \right) - 2K \left( 4, 1 \right) + K \left( 4, 2 \right) -
    2K \left( 1, 4 \right) + 4K \left( 1, 1 \right) - 2K \left( 1, 2 \right) + \\
    + K \left( 2, 4 \right) - 2K \left( 2, 1 \right) + K \left( 2, 2 \right) = \\
    = 4 - 2 \cdot 1 + 2 - 2 \cdot 1 + 4 \cdot 1 - 2 \cdot 1 + 2 - 2 \cdot 1 + 2 =
    4 + 4 - 2 =
    6.
  \end{gather*}

  Нужно найти второй момент.
  $ \xi $ центрирована $M \xi^2 = D \xi = 6$;
  \item $W \left( 1 \right) + 2W \left( 2 \right) + 1 =
    \xi \sim
    N \left( 1, 13 \right) $,
  потому что это линейная комбинация элементов гауссовского вектора.
  Найдём дисперсию.
  Константа на неё не влияет
  $D \xi =
    D \left[ W \left( 1 \right) - 2W \left( 2 \right) \right] =
    cov \left( \xi, \xi \right) $.
  Подставим выражения для случайной величины
  $$cov \left( \xi, \xi \right) =
    cov \left[
      W \left( 1 \right) + 2W \left( 2 \right) + 1,
      W \left( 1 \right) + 2W \left( 2 \right) + 1 \right] =$$
  Воспользуемся линейностью
  $$= K \left( 1, 1 \right) + 2K \left( 1, 2 \right) + 2K \left( 2, 1 \right) +
    4K \left( 2, 2 \right) =
    1 + 2 + 2 + 8 =
    13.$$

  Нужно найти третий момент.
  $ \xi $ не центрирована.
  Нужно её центрировать $M \xi^3 = M \left[ \left( \xi - 1 \right) + 1 \right]^3$.
  Раскрываем скобки
  $$M \left[ \left( \xi - 1 \right) + 1 \right]^3 =
    M \left( \xi - 1 \right)^3 + 3M \left( \xi - 1 \right)^2 + 3M \left( \xi - 1 \right) + 1 =$$
  По задаче 5.2 первое слагаемое --- 0, так как величина центрирована, второй момент --- 13,
  так как это дисперсия, первый момент --- ноль.
  Тогда
  $$= 0 + 3 \cdot 13 + 3 \cdot 0 + 1 = 39 + 1 = 40;$$
  \item $W \left( 3 \right) - 2W \left( 2 \right) = \xi \sim N \left( 0, 3 \right) $,
  потому что это линейная комбинация элементов гауссовского вектора.
  Найдём дисперсию
  $$D \xi =
    D \left[ W \left( 3 \right) - 2W \left( 2 \right) \right] =
    cov \left( \xi, \xi \right) =$$
  Подставим выражение для случайной величины
  $$= cov \left[
      W \left( 3 \right) - 2W \left( 2 \right), W \left( 3 \right) - 2W \left( 2 \right) \right] =$$
  Воспользуемся линейностью
  $$= K \left( 3, 3 \right) - 2K \left( 3, 2 \right) - 2K \left( 2, 3 \right) +
    4K \left( 2, 2 \right) =
    3 - 2 \cdot 2 - 2 \cdot 2 + 4 \cdot 2 =
    3.$$
  Нужно найти
  $$Me^{ \xi } =
    \int \limits_{ \mathbb{R}}
      \frac{1}{ \sqrt{6 \pi }} \cdot e^x \cdot p_{ \xi } \left( x \right) dx =
    \int \limits_{ \mathbb{R}}
      \frac{1}{ \sqrt{6 \pi }} \cdot e^x \cdot e^{- \frac{x^2}{2 \cdot 3}} dx =
    \int \limits_{ \mathbb{R}} \frac{1}{ \sqrt{6 \pi }} \cdot e^{x - \frac{x^2}{6}} dx.$$
  Выделим полный квадрат в степени экспоненты
  $$ \frac{x^2}{6} - x =
    \frac{x^2}{ \left( \sqrt{6} \right)^2} -
    2 \cdot \frac{1}{2} \cdot x \cdot \frac{1}{ \sqrt{6}} \cdot \sqrt{6} +
    \left( \frac{1}{2} \cdot \sqrt{6} \right)^2 - \left( \frac{1}{2} \cdot \sqrt{6} \right)^2 =$$
  Три первых слагаемых образуют полный квадрат
  $$= \left( \frac{x}{ \sqrt{6}} - \frac{ \sqrt{6}}{2} \right)^2 - \frac{3}{2} =
    \frac{ \left( x - 3 \right)^2}{2 \cdot 3} - \frac{3}{2}.$$
  Подставим полученное выражение в экспоненту
  $$ \int \limits_{ \mathbb{R}} \frac{1}{ \sqrt{6 \pi }} \cdot e^{x - \frac{x^2}{6}} dx =
    e^{ \frac{3}{2}}
    \int \limits_{ \mathbb{R}}
      \frac{1}{ \sqrt{6 \pi }} \cdot e^{- \frac{ \left( x - 3 \right)^2}{6}} dx =$$
  Подинтергальная функция --- плотность нормального распределения,
  потому такой интеграл равен единице
  $$= e^{ \frac{3}{2}};$$
  \item нужно найти характеристическую функцию $W \left( 1 \right) + 2W \left( 2 \right) + 1$.

  Математическое ожидание такой величины равно 1, а дисперсия --- 13.
  Значит, получается
  $ \varphi_{W \left( 1 \right) + 2W \left( 2 \right) + 1} \left( \lambda \right) =
    \varphi_{N \left( 1, 13 \right) } \left( \lambda \right) =
    e^{i \lambda - \frac{13 \lambda^2}{2}}$.
\end{enumerate}

\subsubsection*{5.13}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Докажите, что процессы:
\begin{enumerate}[label=\alph*)]
  \item $ \left\{ W \left( T \right) - W \left( T - t \right), \, 0 \leq t \leq T \right\}, \,
    T = const > 0$;
  \item $ \left\{ \sqrt{c} W \left( \frac{t}{c} \right), \, t \geq 0 \right\}, \,
    c = const > 0$
\end{enumerate}
тоже являются винеровскими.

\textit{Решение.}
$ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- это винеровский процесс.
Нужно проверить, что некоторые преобразования винеровского процесса оставляют его винеровским.
\begin{enumerate}[label=\alph*)]
  \item Сначала нужно сказать, что у процесса гауссовские конечномерные распределения.

  Берём $n$ значений этого процесса
  $$ \left( W \left( T \right) - W \left( T - t_1 \right), \dotsc,
    W \left( T \right) - W \left( T - t_n \right) \right) $$
  --- гауссовский, так как этот вектор --- это линейное преобразование вектора
  $ \left( W \left( T - t_1 \right), \dotsc, W \left( T - t_n \right), W \left( T \right) \right) $.

  Что это будет за линейное преобразование?
  $$ \begin{bmatrix}
      -1 & 0 & 1 \\
      0 & -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      W \left( T - t_1 \right) \\
      W \left( T - t_2 \right) \\
      W \left( T \right)
    \end{bmatrix} =
    \begin{bmatrix}
      W \left( T \right) - W \left( T - t_1 \right) \\
      W \left( T \right) - W \left( T - t_2 \right)
    \end{bmatrix}.$$

  Математическое ожидание --- 0.

  Нужно посчитать ковариационную функцию.
  Нужно проверить, что она равняется минимуму
  $$K \left( t, s \right) =
    M \left\{
      \left[ W \left( T \right) - W \left( T - t \right) \right] \cdot
      \left[ W \left( T \right) - W \left( T - s \right) \right] \right\} =$$
  Перемножим скобки
  $$= M \left[
      W \left( T \right)^2 - W \left( T \right) W \left( T - s \right) -
      W \left( T - t \right) W \left( T \right) + W \left( T - t \right) W \left( T - s \right)
    \right] =$$
  Математическое ожидание трёх последних слагаемых --- ковариационные функции винеровского процесса.
  Они равны минимуму.
  Математическое ожидание первого слагаемого --- ковариация в точке $ \left( T, T \right) $.
  Получаем
  $$= T - T + s - T + t + \min \left( T - s, T - t \right) =
    s + t - \max \left( s, t \right) =
    \min \left( t, s \right).$$

  Значит, ковариация такая, как надо.
  Это винеровский процесс;
  \item берём конечномерные распределения
  $$ \left(
      \sqrt{c} W \left( \frac{t_1}{c} \right), \dotsc, \sqrt{x} W \left( \frac{t_n}{c} \right)
    \right) $$
  --- гауссовский, так как это линейное преобразование вектора винеровского процесса
  $$ \left( W \left( \frac{t_1}{c} \right), \dotsc, W \left( \frac{t_n}{c} \right) \right).$$

  Математическое ожидание --- 0.
  Осталось найти ковариационную функцию
  $$K \left( t, s \right) =
    M \left[ \sqrt{c} W \left( \frac{t}{c} \right) \sqrt{c} W \left( \frac{s}{c} \right) \right] =$$
  Выносим $ \sqrt{c}$.
  Получим
  $$= cM \left[ W \left( \frac{t}{c} \right) W \left( \frac{s}{c} \right) \right] =
  c \cdot \min \left( \frac{t}{c}, \frac{s}{c} \right) =$$
  Множитель $c$ --- положительный.
  Он вносится
  $$= \min \left( t, s \right).$$
\end{enumerate}

\subsubsection*{5.14}

\textit{Задание.}
Для фиксированного $ \rho \in \left[ -1, 1 \right] $ положим
$$W \left( t \right) = \rho W^1 \left( t \right) + \sqrt{1 - \rho^2} W^2 \left( t \right),$$
где
$ \left\{ W^1 \left( t \right), \, t \geq 0 \right\}, \,
  \left\{ W^2 \left( t \right), \, t \geq 0 \right\} $
--- независимые винеровские процессы.
Докажите, что процесс $ \left\{ W \left( t \right), \, t \geq 0 \right\} $
является винеровским и найдите математическое ожидание
$M \left[ W^1 \left( t \right) \cdot W \left( t \right) \right] $.

\textit{Решение.}
Сложили 2 независимых винеровских процесса так, чтобы процесс был винеровским.

Скажем, что такой процесс гауссовский
\begin{gather*}
  \left( W \left( t_1 \right), \dotsc, W \left( t_n \right) \right) = \\
  = \left(
    \rho W^1 \left( t_1 \right) + \sqrt{1 - \rho^2} W^2 \left( t_1 \right), \dotsc,
    \rho W^1 \left( t_n \right) + \sqrt{1 - \rho^2} W^2 \left( t_n \right) \right)
\end{gather*}
--- это линейное преобразование
$ \left( W^1 \left( t_1 \right), \dotsc, W^1 \left( t_n \right) \right) $ и
$$ \left( W^2 \left( t_1 \right), \dotsc, W^2 \left( t_n \right) \right) $$
--- гауссовские вектора.

Математическое ожидание такого процесса --- 0, так как математическое ожидание каждого процесса ---
0.
Посчитаем ковариацию.
Ковариация линейна по каждому аргументу
\begin{gather*}
  cov \left[
    \rho W^1 \left( t \right) + \sqrt{1 - \rho^2} W^2 \left( t \right), \,
    \rho W^1 \left( s \right) + \sqrt{1 - \rho^2} W^2 \left( s \right) \right] = \\
  = \rho^2 \cdot cov \left[ W^1 \left( t \right), W^1 \left( s \right) \right] +
  \rho \sqrt{1 - \rho^2} \cdot cov \left[ W^1 \left( t \right), W^2 \left( s \right) \right] + \\
  + \sqrt{1 - \rho^2} \cdot \rho \cdot
  cov \left[ W^2 \left( t \right), W^1 \left( s \right) \right] +
  \left( 1 - \rho^2 \right) \cdot cov \left[ W^2 \left( t \right), W^2 \left( s \right) \right] =
\end{gather*}
Когда индексы разные --- это 0, когда одинаковые --- это минимум
$$= \rho^2 \cdot \min \left( t, s \right) +
  \left( 1 - \rho^2 \right) \cdot \min \left( t, s \right) =
  \min \left( t, s \right),$$
тогда процесс винеровский.

$$M \left[ W^1 \left( t \right) \cdot W \left( t \right) \right] =
  M \left\{
    W^1 \left( t \right) \cdot
    \left[ \rho W^1 \left( t \right) + \sqrt{1 - \rho^2} W^2 \left( t \right) \right] \right\} =$$
Раскроем скобки
$$= \rho MW^1 \left( t \right)^2 +
  \sqrt{1 - \rho^2} M \left[ W^1 \left( t \right) W^2 \left( t \right) \right] =
  \rho t.$$

\subsubsection*{5.15}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Найдите математическое ожидание и ковариационную функцию процесса
$$X \left( t \right) =
  x + \mu t + \sigma W \left( t \right),$$
который называется винеровским со сдвигом $ \mu \in \mathbb{R}$,
коэффициентом диффузии $ \sigma > 0$, который стартует из точки $x \in \mathbb{R}$.

\textit{Решение.}
Из определения винеровского процесса следует, что случайная величина $W \left( t \right) $
имеет нормальное распределение с нулевым математическим ожиданием и дисперсией $t$.
Таким образом
$$MX \left( t \right) =
  M \left[ x + \mu t +  \sigma W \left( t \right) \right] =
  Mx + M \left( \mu t \right) + \sigma MW \left( t \right) =
  x + \mu t.$$

Вычислим теперь $MX \left( t \right) X \left( s \right) $.
Имеем
$$MX \left( t \right) X \left( s \right) =
  M \left\{
    \left[ x + \mu t + \sigma W \left( t \right) \right] \cdot
    \left[ x + \mu s + \sigma W \left( s \right) \right] \right\} =$$
Перемножим скобки
\begin{gather*}
  = M[
    x^2 + x \mu s + x \sigma W \left( s \right) + \mu tx + \mu^2 ts +
    \mu t \sigma W \left( s \right) + \sigma W \left( t \right) x +
    \sigma W \left( t \right) \mu s + \\
    + \sigma^2 W \left( t \right) W \left( s \right) ] =
\end{gather*}
Математическое ожидание константы --- это сама константа,
а математическое ожидание винеровского процесса равно нулю
$$= x^2 + x \mu s + \mu tx + \mu^2 ts+ \sigma^2 \cdot \min \left( t, s \right).$$

Тогда
$$cov \left[ X \left( t \right), X \left( s \right) \right] =
  M \left[ X \left( t \right) X \left( s \right) \right] -
  MX \left( t \right) \cdot MX \left( s \right) =$$
Подставим найденные выражения для математических ожиданий
$$= x^2 + x \mu s + \mu tx + \mu^2 ts + \sigma^2 \cdot \min \left( t, s \right) -
  \left( x + \mu t \right) \left( x + \mu s \right) =$$
Перемножим скобки
$$= x^2 + x \mu s + \mu ts + \mu^2 ts + \sigma^2 \cdot \min \left( t, s \right) - x^2 - x \mu s -
  \mu tx - \mu^2 ts =$$
Сократим
$$= \sigma^2 \cdot \min \left( t, s \right).$$

\subsubsection*{5.16}

\textit{Задание.}
Докажите, что случайный процесс
$$Z \left( t \right) = tW \left( \frac{1}{t} - 1 \right), \,
  0 < t \leq 1; \,
  Z \left( 0 \right) = 0$$
имеет то же распределение, что и броуновский мост.

\textit{Решение.}
Конечномерные распределения такого процесса
$$ \left( Z \left( t_1 \right), \dotsc, Z \left( t_n \right) \right) $$
--- гауссовские вектора,
потому что это линейное преобразование винеровского процесса.

$MZ \left( t \right) =
  0$.

Найдём ковариацию
$$cov \left[ Z \left( t \right), Z \left( s \right) \right] =
  cov \left[ tW \left( \frac{1}{t} - 1 \right), sW \left( \frac{1}{s} - 1 \right) \right] =$$
Множители выносим
$$= ts \cdot \min \left( \frac{1}{t} - 1, \frac{1}{s} - 1 \right) =$$
Вносим положительный множитель в минимум
$$= \min \left( s - ts, t - ts \right) =$$
Общее выносим за минимум
$$= \min \left( t, s \right) - ts,$$
то есть ковариация такая же, как и у броуновского моста.


\subsubsection*{5.17}

\textit{Задание.}
Пусть $ \left\{ W \left( t \right), \, t \geq 0 \right\} $ --- винеровский процесс.
Вычислите условное математическое ожидание
$M \left( \left. W \left( s \right) \right| W \left( t \right) \right) $ при $s < t$.

\textit{Решение.}
\begin{gather*}
  M \left[ W \left( s \right) \; \middle| \; W \left( t \right) \right] = \\
  = MW \left( s \right) +
  cov \left[ W \left( s \right), W \left( t \right) \right] \cdot
  cov^{-1} \left[ W \left( t \right), W \left( t \right) \right] \cdot
  \left[ W \left( t \right) - MW \left( t \right) \right]=
\end{gather*}
Математическое ожидание винеровского процесса равно нулю, а ковариация --- минимуму
$$= \min \left( s, t \right) \cdot \frac{1}{t} \cdot W \left( t \right) =$$
По условию $s < t$, потому
$$= \frac{s}{t} \cdot W \left( t \right).$$

\subsubsection*{5.18}

\textit{Задание.}
Пусть $W$ и $N$ --- независимые между собой винеровский процесс и пуассоновский процесс с
интенсивностью $ \lambda $ соответственно.
Найдите характеристическую функцию случайной величны
$X \left( t \right) = W \left( N \left( t \right) \right) $.

\textit{Решение.}
Нужно найти $ \varphi_{W \left( N \left( t \right) \right) }$.
Характеристическая функция --- это $Me^{isW \left( N \left( t \right) \right) }$.
Имеемв винеровский процесс, в который подставляется случайное время.
Нужно перебрать все возможные значения случайного времени.
Пуассоновский процесс принимает значения от нуля до бесконечности
$$Me^{isW \left( N \left( t \right) \right) } =
  M \sum \limits_{k = 0}^{ \infty }
    \mathbbm{1} \left\{ N \left( t \right) = k \right\} \cdot e^{isW \left( k \right) } =$$
Математическое ожидание суммы можем написать как сумму математических ожиданий
$$= \sum \limits_{k = 0}^{ \infty }
  M \mathbbm{1} \left\{ N \left( t \right) = k \right\} e^{isW \left( k \right) } =$$
Индикатор зависит от пуассоновского процесса, а экспонента --- от винеровского, а они независимы
$$= \sum \limits_{k = 0}^{ \infty }
  P \left\{ N \left( t \right) = k \right\} Me^{isW \left( k \right) } =$$
Оба множителя нам известны.
Второй --- это характеристическая функция гауссовской величины
$$= \sum \limits_{k = 0}^{ \infty } e^{- \lambda t} \cdot
\frac{ \left( \lambda t \right)^k}{k!} \cdot e^{-\frac{ \lambda^2}{2} \cdot k} =$$
Случайная величина
$N \left( t \right) \sim Pois \left( \lambda t \right), \,
  W \left( k \right) \sim N \left( 0, k \right) $.
Получаем
$$= e^{- \lambda t} \sum \limits_{k = 0}^{ \infty } \frac{ \left( \lambda t e^{- \frac{s^2}{2}} \right)^k}{k!} =
  e^{ \lambda t \left( e^{-\frac{s^2}{2}} - 1 \right) }.$$

Тогда
$Me^{isW \left( N \left( t \right) \right) } =
  M \left. \left( Me^{isW \left( t \right) } \right) \right|_{k = N \left( t \right) } =
  M \left[
    e^{isW \left( N \left( t \right) \right) } \; \middle| \; N \left( t \right) = k \right] $.

Свойство условного математического ожидания
$MM \left( \xi \; \middle| \; \mathcal{F} \right) =
  M \xi $.
